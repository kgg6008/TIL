{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어와 단어 분산 표현\n",
    "---\n",
    "자연어 처리가 다루는 분야는 다양하지만, 그 본질적 문제는 컴퓨터가 우리의 말을 이해하게 만드는 것이다.  \n",
    "이번 Chapter는 컴퓨터에 말을 이해시킨다는 것이 무슨 뜻인지, 그리고 어떤 방법들이 존재하는지를 중심적으로 진행한다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 자연어 처리란?\n",
    "- 자연어(natural language) : 한국어, 영어 등 우리가 평소에 사용하는 말\n",
    "- NLP(Natural Language Processing) : 자연어를 처리하는 분야 ( 자연어를 컴퓨터에게 이해시키기 위한 기술(분야) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 시소러스(Thesaurus)\n",
    "- 시소러스란?\n",
    "    - 유의어 사전으로, '뜻이 같은 단어(동의어)'나 '뜻이 비슷한 단어(유의어)'가 한 그룹으로 분류되어 있다.\n",
    "    - ex. car = 'auto', 'automobile', 'machine', 'motorcar'\n",
    "- 시소러스에서는 단어 사이의 '상위와 하위' 혹은 '전체와 부분' 등, 더 세세한 관계까지 정의해둔 경우가 있다.\n",
    "    - ex. object - motor vehicle - car - SUV, compact, hatch-back...\n",
    "\n",
    "#### WordNet\n",
    "- 자연어 처리 분야에서 가장 유명한 시소러스\n",
    "- 프린스턴 대학교에서 1985년부터 구축하기 시작한 전통 있는 시소러스\n",
    "- WordNet을 사용하면 유의어를 얻거나 단어 네트워크를 이용할 수 있다.\n",
    "    - 이를 이용하여 단어 사이의 유사도도 구할 수있다.\n",
    "    \n",
    "#### 시소러스의 문제점\n",
    "- 시대 변화에 대응하기 어렵다.\n",
    "    - 우리가 사용하는 말은 계속해서 새로운 단어가 생기고 옛말은 언젠가 잊혀진다.\n",
    "    - 단어의 변화에 대응하려면 시소러스를 사람이 수작업으로 끊임 없이 갱신해야 한다.\n",
    "- 사람을 쓰는 비용은 크다.\n",
    "    - 시소러스를 만드는 데는 엄청난 인적 비용이 발생한다.\n",
    "    - 현존하는 영어의 수는 1,000만개가 넘는데 이 방대한 단어를 모두에 대해 단어 사이의 관계를 정의해야 한다.\n",
    "    - WordNet에 등록된 단어는 20만 개 이상이다.\n",
    "- 단어의 미묘한 차이를 표현할 수 없다.\n",
    "    - 시소러스는 뜻이 비슷한 단어를 묶으나 실제로 비슷한 단어들이라도 미묘한 차이가 있다.\n",
    "    - 시소러스에는 미묘한 차이를 표현할 수 없다. 만약 수작업으로 표현하려 한다면 곤란한 일이 될 것이다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 통계 기반 기법\n",
    "- 말뭉치(Corpus)란, 간단히 말하면 대량의 텍스트 데이터이다.\n",
    "    - 맹목적으로 수집된 데이터가 아닌 자연어 처리 연구나 애필리케이션을 염두에 두고 수집된 텍스트 데이터를 일반적으로 말뭉치라고 한다.\n",
    "- 자연어 처리에 사용되는 말뭉치에는 텍스트 데이터에 대한 추가 정보가 포함되는 경우가 있다.\n",
    "    - 예를 들면 텍스트 데이터의 단어 각각에 품사 레이블링\n",
    "      \n",
    "#### Python으로 말뭉치 전처리 하기\n",
    "- 자연어 처리에는 다양한 말뭉치가 사용된다.\n",
    "    - 유명한 것 : 위키백과, 구글 뉴스, 셰익스피어나 나쓰메 소세키 같은 대문호의 작품\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin >> You say goodbye and I  say hello.\n",
      "Preprocessed >> you say goodbye and i  say hello .\n",
      "Word >> ['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']\n"
     ]
    }
   ],
   "source": [
    "# 간단한 말뭉치를 이용한 corpus 전처리 예시\n",
    "text = 'You say goodbye and I  say hello.'\n",
    "print(f'Origin >> {text}')\n",
    "#소문자로 변환\n",
    "text = text.lower()\n",
    "# 마침표의 경우를 고려\n",
    "text = text.replace('.', ' .')\n",
    "print(f'Preprocessed >> {text}')\n",
    "words = text.split()\n",
    "print(f'Word >> {words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래의 문장에서 단어에 ID를 부여하고, ID의 리스트로 이용할 수 있도록 손질\n",
    "# 단어 ID에서 단어로의 변환은 id_to_word\n",
    "# 단어에서 단어 ID로의 변환은 word_to_id\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 1, 5, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess함수로 전처리 진행\n",
    "def preprocess(text):\n",
    "    text = text.lower().replace('.',' .')\n",
    "    words = text.split()\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 1, 5, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어의 분산 표현(distributional representation)\n",
    "- 단어의 의미를 정확하게 파악할 수 있게 단어를 벡터로 표현하는 것\n",
    "\n",
    "#### 분포 가설(distributional hypothesis)\n",
    "- 단어의 의미는 주변 단어에 의해 형성된다.\n",
    "- 단어 자체에는 의미가 없고, 그 단어가 사용된 맥락(문맥, context)이 형성한다.\n",
    "- 맥락의 크기(주변 단어를 몇 개나 포함할지)를 윈도우 크기라고 한다.\n",
    "\n",
    "#### 동시발생 행렬\n",
    "- 어떤 단어가 있을 때, 그 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하는 방식을 통계적 기반 기법이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "            \n",
    "            if left_idx >= 0 :\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_co_matrix(corpus, len(word_to_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 벡터 간 유사도\n",
    "- 단어 벡터의 유사도를 나타낼 때는 코사인 유사도(cosine similarity)를 사용한다.\n",
    "- $$similarity(x, y) = \\frac{x \\cdot y } {||x||||y||} = \\frac {x_{1}y_{1} + \\cdots x_{n}y_{n}} {\\sqrt {x_{1}^{2} + \\cdots x_{n}^{2}}\\sqrt {y_{1}^{2} + \\cdots y_{n}^{2}}}$$\n",
    "    - 코사인 유사도를 직관적으로 풀어보면 '두 벡터가 가리키는 방향이 얼마나 비슷한가'이다.\n",
    "    - 두 벡터의 방향이 완전히 같다면 코사인 유사도가 1이 되며, 완전히 반대라면 -1이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도\n",
    "# eps는 divide by zero를 위한 오차\n",
    "def cos_similarity(x, y, eps=1e-10):\n",
    "    nx = x / np.sqrt(np.sum(x**2) + eps)\n",
    "    ny = y / np.sqrt(np.sum(y**2) + eps)\n",
    "    return np.dot(nx,ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811335144"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "cos_similarity(C[word_to_id['you']], C[word_to_id['i']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유사 단어의 랭킹 표시\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    if query not in word_to_id:\n",
    "        print(f'{query}을(를) 찾을 수 없습니다.')\n",
    "        return\n",
    "    \n",
    "    print('\\n[query] ' + query )\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "    \n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "        \n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(f' {id_to_word[i]} : {similarity[i]}')\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye : 0.7071067811335144\n",
      " i : 0.7071067811335144\n",
      " hello : 0.7071067811335144\n",
      " say : 0.0\n",
      " and : 0.0\n"
     ]
    }
   ],
   "source": [
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 통계 기반 기법 개선하기\n",
    "\n",
    "#### 상호정보량\n",
    "- 점별 상호정보량(Pointwise Mutual information, PMI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
